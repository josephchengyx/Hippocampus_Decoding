{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from scipy.io import loadmat\n",
    "import h5py\n",
    "from preproc import *\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify which days' dataset to use\n",
    "prefix = \"/Volumes/Hippocampus/Data/picasso-misc/\"\n",
    "sessions = [\"20181102\", \"20181101\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_data(day_dir: str):\n",
    "    # Get list of cells under the day directory\n",
    "    os.system(f\"sh ~/Documents/neural_decoding/Hippocampus_Decoding/get_cells.sh {day_dir}\")\n",
    "    cell_list = list()\n",
    "    with open(\"cell_list.txt\", \"r\") as file:\n",
    "        for line in file.readlines():\n",
    "            cell_list.append(line.strip())\n",
    "    os.system(\"rm cell_list.txt\")\n",
    "\n",
    "    # Load data from rplparallel.mat object, extract trial markers, time stamps and session start timestamp\n",
    "    rp = h5py.File(prefix + day_dir + \"/session01/rplparallel.mat\")\n",
    "    rp = rp.get('rp').get('data')\n",
    "    trial_markers = np.array(rp.get('markers'))\n",
    "    trial_timestamps = np.array(rp.get('timeStamps'))\n",
    "    session_start_time = np.round(np.array(rp.get('session_start_sec'))[0,0], 3)\n",
    "\n",
    "    # Load data and extract spike times from all spiketrain.mat objects\n",
    "    spike_times = list()\n",
    "    cell_labels = list()\n",
    "    for cell_dir in cell_list:\n",
    "        spk = loadmat(prefix + day_dir + \"/session01/\" + cell_dir + \"/spiketrain.mat\")\n",
    "        spk = spk.get('timestamps').flatten() # spike timestamps is loaded in as a column vector\n",
    "        spk = spk / 1000 # convert spike timestamps from msec to sec\n",
    "        spike_times.append(spk)\n",
    "        \n",
    "        cell_name = cell_dir.split('/')\n",
    "        array, channel, cell = cell_name[0][6:], cell_name[1][7:], cell_name[2][5:]\n",
    "        if channel[0] == '0':\n",
    "            channel = channel[1:]\n",
    "        cell_labels.append(f'{day_dir}/ch{channel}/c{cell}')\n",
    "\n",
    "    # Load data from vmpv.mat object, extract session end timestamp\n",
    "    pv = h5py.File(prefix + day_dir + \"/session01/1vmpv.mat\")\n",
    "    pv = pv.get('pv').get('data')\n",
    "    session_end_time = np.round(np.array(pv.get('rplmaxtime'))[0,0], 3)\n",
    "\n",
    "    return trial_markers, trial_timestamps, session_start_time, session_end_time, spike_times, cell_labels\n",
    "\n",
    "\n",
    "def session_preproc(session_data):\n",
    "    # Unpack session data\n",
    "    trial_markers, trial_timestamps, session_start_time, session_end_time, spike_times, cell_labels = session_data\n",
    "\n",
    "    # Get poster numbers from trial markers, cue phase time intervals\n",
    "    trial_markers = trial_markers[0,:] % 10\n",
    "    trial_markers = trial_markers.astype(int)\n",
    "    cue_intervals = trial_timestamps[0:2,:].T\n",
    "\n",
    "    # Get durations of each navigation phase\n",
    "    nav_intervals = trial_timestamps[1:,:].T\n",
    "    nav_durations = nav_intervals[:,1] - nav_intervals[:,0]\n",
    "    nav_durations = nav_durations.astype(np.int8)\n",
    "\n",
    "    # Generate time intervals for each trial\n",
    "    trial_intervals = np.empty_like(cue_intervals)\n",
    "    trial_intervals[:,0] = cue_intervals[:,1]\n",
    "    trial_intervals[:-1,1] = cue_intervals[1:,0]\n",
    "    trial_intervals[-1,1] = session_end_time\n",
    "\n",
    "    # Further differentiate trial markers into trial trajectories (start poster, end poster)\n",
    "    trial_trajectories = np.zeros((trial_markers.shape[0], 2))\n",
    "    trial_trajectories[:,1] = trial_markers\n",
    "    trial_trajectories[1:,0] = trial_markers[:-1]\n",
    "\n",
    "    # Filter out trials that are too long (> 25 seconds) or have repeated goal from previous trial\n",
    "    good_trials = np.ones(trial_markers.shape, dtype=np.int8)\n",
    "    max_dur = 25  # maximum duration of trials (in seconds) to filter out\n",
    "    prev_goal = 0\n",
    "    for num, dur in enumerate(nav_durations):\n",
    "        curr_goal = trial_markers[num]\n",
    "        if dur > max_dur or curr_goal == prev_goal:\n",
    "            good_trials[num] = 0\n",
    "        prev_goal = curr_goal\n",
    "    good_trials[0] = 0  # Discard the first trial also\n",
    "    trial_filt = np.where(good_trials == 1)\n",
    "\n",
    "    trial_markers = trial_markers[trial_filt]\n",
    "    trial_trajectories = trial_trajectories[trial_filt,:][0]  # not sure why it adds an extra axis\n",
    "    cue_intervals = cue_intervals[trial_filt,:][0]  # not sure why it adds an extra axis\n",
    "    nav_intervals = nav_intervals[trial_filt,:][0]  # not sure why it adds an extra axis\n",
    "    trial_intervals = trial_intervals[trial_filt]\n",
    "\n",
    "    return trial_markers, trial_trajectories, cue_intervals, nav_intervals, trial_intervals, spike_times, cell_labels\n",
    "\n",
    "\n",
    "def spiketrain_preproc(session_data, timebin_window: int):\n",
    "    # Unpack session data\n",
    "    trial_markers, trial_trajectories, cue_intervals, nav_intervals, trial_intervals, spike_times, cell_labels = session_data\n",
    "    # Get number of cells in dataset\n",
    "    num_cells = len(cell_labels)\n",
    "\n",
    "    # Bin entire session into 250 ms time bins, aligned to the start of each cue phase for each trial\n",
    "    session_intervals = list()\n",
    "    delta = 0.25  # Size of time bin (in seconds)\n",
    "    for idx, intvl in enumerate(trial_intervals):\n",
    "        trial_start, trial_end = intvl\n",
    "        for time in np.arange(trial_start, trial_end - delta, delta):\n",
    "            session_intervals.append(np.array([time, time + delta]))\n",
    "    session_intervals = np.array(session_intervals)\n",
    "\n",
    "    # Divide cue phases into 250 ms time bins\n",
    "    num_prds = int(1/delta)\n",
    "    new_cue_intervals = np.empty((cue_intervals.shape[0], cue_intervals.shape[1], num_prds))\n",
    "    for num, intvl in enumerate(cue_intervals):\n",
    "        st_time, ed_time = intvl\n",
    "        for prd in range(num_prds):\n",
    "            new_cue_intervals[num,0,prd] = st_time + delta * prd\n",
    "            new_cue_intervals[num,1,prd] = st_time + delta * (prd + 1)\n",
    "    full_cue_intervals = cue_intervals\n",
    "    cue_intervals = new_cue_intervals\n",
    "\n",
    "    # Choose which 250ms timebin to use for cue intervals to be fitted to the model\n",
    "    cue_intervals = cue_intervals[:,:,timebin_window]\n",
    "    timebin_labels = ['0-250ms', '250-500ms', '500-750ms', '750ms-1s']\n",
    "\n",
    "    # Slot spikes into cue phase intervals for each trial and session time intervals\n",
    "    spikerates_cue = spike_rates_per_observation(cue_intervals, spike_times)\n",
    "    spikerates_session = spike_rates_per_observation(session_intervals, spike_times)\n",
    "\n",
    "    # Bin spike rates within each cell for entire sesion, and get firing rate thresholds used for binning\n",
    "    binned_spikes_session = np.empty_like(spikerates_session)\n",
    "    binning_stats = list()\n",
    "    for col in range(spikerates_session.shape[1]):\n",
    "        binned_spikes_session[:,col] = bin_firing_rates_4(spikerates_session[:,col])\n",
    "        binning_stats.append(get_binning_stats_4(spikerates_session[:,col]))\n",
    "    \n",
    "    # Bin spike rates within each cell for cue phases\n",
    "    binned_spikes_cue = np.empty_like(spikerates_cue)\n",
    "    for col in range(spikerates_cue.shape[1]):\n",
    "        binned_spikes_cue[:,col] = bin_firing_rates_4(spikerates_cue[:,col], stats=binning_stats[col])\n",
    "    \n",
    "    return trial_markers, trial_trajectories, spikerates_cue, binned_spikes_cue, cell_labels\n",
    "\n",
    "\n",
    "def groupby_trial_trajectories(timeseries: np.array, trial_trajectories: np.array) -> dict:\n",
    "    # Some important constants\n",
    "    num_cells = timeseries.shape[1]\n",
    "    num_goals = 6\n",
    "    # Group responses according to trial trajectories\n",
    "    trial_responses = dict()\n",
    "    for idx, trial in enumerate(timeseries):\n",
    "        traj = tuple(trial_trajectories[idx])\n",
    "        if traj not in trial_responses:\n",
    "            trial_responses[traj] = trial\n",
    "        else:\n",
    "            trial_responses[traj] = np.vstack((trial_responses[traj], trial))\n",
    "    return trial_responses\n",
    "\n",
    "\n",
    "def random_pairings(list_lengths: list) -> np.array:\n",
    "        min_len = min(list_lengths)\n",
    "        indices = np.hstack([np.random.permutation(range(l))[:min_len].reshape((-1,1)) for l in list_lengths])\n",
    "        return indices\n",
    "\n",
    "\n",
    "def merge_sessions(sessions_data: list):\n",
    "    # Contents of each session's data:\n",
    "    # (0) trial_markers, (1) trial_trajectories, (2) spikerates_cue, (3) binned_spikes_cue, (4) cell_labels\n",
    "    # raw/binned_responses_grouped is a list of dictionaries, each dictionary corresponds to the grouped responses for one session\n",
    "    num_sessions = len(sessions_data)\n",
    "    cell_labels = [cell for sess in sessions_data for cell in sess[4]]\n",
    "    trajectories = [sess[1] for sess in sessions_data]\n",
    "    raw_responses = [sess[2] for sess in sessions_data]\n",
    "    binned_responses = [sess[3] for sess in sessions_data]\n",
    "    raw_responses_grouped = list()\n",
    "    binned_responses_grouped = list()\n",
    "    for sess, traj in enumerate(trajectories):\n",
    "        raw_resp = raw_responses[sess]\n",
    "        binned_resp = binned_responses[sess]\n",
    "        raw_responses_grouped.append(groupby_trial_trajectories(raw_resp, traj))\n",
    "        binned_responses_grouped.append(groupby_trial_trajectories(binned_resp, traj))\n",
    "\n",
    "    # Get number of responses per trial trajectory for each session \n",
    "    num_responses_per_traj = [dict(map(lambda item: (item[0], item[1].shape[0]), sess.items())) for sess in raw_responses_grouped]\n",
    "    trial_types = list(num_responses_per_traj[0].keys())\n",
    "    merged_raw_responses = dict()\n",
    "    merged_binned_responses = dict()\n",
    "    for traj in trial_types:\n",
    "        # Get number of trials from each session for the given trial trajectory\n",
    "        num_trials = [sess[traj] for sess in num_responses_per_traj]\n",
    "        pairings = random_pairings(num_trials)  # Generate pairings between sessions for the given trial trajectory\n",
    "        # Concatenate responses across sessions according to generated pairings\n",
    "        raw_resp = [sess[traj] for sess in raw_responses_grouped]\n",
    "        binned_resp = [sess[traj] for sess in binned_responses_grouped]\n",
    "        merged_raw = np.hstack([raw_resp[sess][pairings[:,sess],:] for sess in range(num_sessions)])\n",
    "        merged_binned = np.hstack([binned_resp[sess][pairings[:,sess],:] for sess in range(num_sessions)])\n",
    "        merged_raw_responses[traj] = merged_raw\n",
    "        merged_binned_responses[traj] = merged_binned\n",
    "\n",
    "    return merged_raw_responses, merged_binned_responses, cell_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import and preprocess data\n",
    "sessions_data = list()\n",
    "for sess in sessions:\n",
    "    sess_data = import_data(sess)\n",
    "    sess_data = session_preproc(sess_data)\n",
    "    sess_data = spiketrain_preproc(sess_data, 1)  # using timebin window of 1s\n",
    "    sessions_data.append(sess_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge data across sessions\n",
    "merged_sessions_data = merge_sessions(sessions_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "decoding",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c3fe2a2c8d7eb9520fbb1c58f5c7303a51cf224a80a092f134560e88ca87af0b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
