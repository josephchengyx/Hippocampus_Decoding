{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from scipy.io import loadmat\n",
    "import h5py\n",
    "from preproc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify which day's dataset to use\n",
    "prefix = \"/Volumes/Hippocampus/Data/picasso-misc/\"\n",
    "day_dir = \"20181102\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get list of cells under the day directory\n",
    "os.system(f\"sh ~/Documents/neural_decoding/Hippocampus_Decoding/get_cells.sh {day_dir}\")\n",
    "cell_list = list()\n",
    "with open(\"cell_list.txt\", \"r\") as file:\n",
    "    for line in file.readlines():\n",
    "        cell_list.append(line.strip())\n",
    "os.system(\"rm cell_list.txt\")\n",
    "\n",
    "# Load data from rplparallel.mat object, extract trial markers, time stamps and session start timestamp\n",
    "rp = h5py.File(prefix + day_dir + \"/session01/rplparallel.mat\")\n",
    "rp = rp.get('rp').get('data')\n",
    "trial_markers = np.array(rp.get('markers'))\n",
    "cue_intervals = np.array(rp.get('timeStamps'))\n",
    "session_start_time = np.round(np.array(rp.get('session_start_sec'))[0,0], 3)\n",
    "\n",
    "# Load data and extract spike times from all spiketrain.mat objects\n",
    "spike_times = list()\n",
    "cell_labels = list()\n",
    "for cell_dir in cell_list:\n",
    "    spk = loadmat(prefix + day_dir + \"/session01/\" + cell_dir + \"/spiketrain.mat\")\n",
    "    spk = spk.get('timestamps').flatten() # spike timestamps is loaded in as a column vector\n",
    "    spk = spk / 1000 # convert spike timestamps from msec to sec\n",
    "    spike_times.append(spk)\n",
    "    \n",
    "    cell_name = cell_dir.split('/')\n",
    "    array, channel, cell = cell_name[0][6:], cell_name[1][7:], cell_name[2][5:]\n",
    "    if channel[0] == '0':\n",
    "        channel = channel[1:]\n",
    "    cell_labels.append(f'a{array}/ch{channel}/c{cell}')\n",
    "\n",
    "# Load data from vmpv.mat object, extract session end timestamp\n",
    "pv = h5py.File(prefix + day_dir + \"/session01/1vmpv.mat\")\n",
    "pv = pv.get('pv').get('data')\n",
    "session_end_time = np.round(np.array(pv.get('rplmaxtime'))[0,0], 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get poster numbers from trial markers, cue phase time intervals\n",
    "trial_markers = trial_markers[0,:] % 10\n",
    "trial_markers = trial_markers.astype(int)\n",
    "cue_intervals = cue_intervals[0:2,:].T\n",
    "\n",
    "# Slot spikes into cue phase intervals for each trial\n",
    "spikecounts_per_trial = spike_counts_per_observation(cue_intervals, spike_times)\n",
    "\n",
    "# Bin spike counts within each cell for cue phases\n",
    "binned_spikes_per_trial = np.empty_like(spikecounts_per_trial)\n",
    "for col in range(spikecounts_per_trial.shape[1]):\n",
    "    binned_spikes_per_trial[:,col] = bin_firing_rates_3(spikecounts_per_trial[:,col])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Bin entire session into 1-second time bins\n",
    "# session_intervals = np.arange(session_start_time, session_end_time, 1)\n",
    "# session_intervals = np.vstack((session_intervals[:-1], session_intervals[1:])).T\n",
    "\n",
    "# Generate time intervals for navigation phases\n",
    "nav_intervals = np.empty_like(cue_intervals)\n",
    "nav_intervals[:,0] = cue_intervals[:,1]\n",
    "nav_intervals[:-1,1] = cue_intervals[1:,0]\n",
    "nav_intervals[-1,1] = session_end_time\n",
    "\n",
    "# Bin entire session into 1-second time bins, aligned to the end of each cue phase for each trial\n",
    "session_intervals = list()\n",
    "for idx, intvl in enumerate(nav_intervals):\n",
    "    session_intervals.append(cue_intervals[idx,:])\n",
    "    nav_start, nav_end = intvl\n",
    "    for time in np.arange(nav_start, nav_end - 1, 1):\n",
    "        session_intervals.append(np.array([time, time + 1]))\n",
    "session_intervals = np.array(session_intervals)\n",
    "\n",
    "# Slot spikes into session time intervals\n",
    "spikecounts_across_session = spike_counts_per_observation(session_intervals, spike_times)\n",
    "\n",
    "# Bin spike counts within each cell for entire sesion\n",
    "binned_spikes_across_session = np.empty_like(spikecounts_across_session)\n",
    "for col in range(spikecounts_across_session.shape[1]):\n",
    "    binned_spikes_across_session[:,col] = bin_firing_rates_3(spikecounts_across_session[:,col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_by_goal(timeseries: np.array, goals: np.array) -> list:\n",
    "    num_goals = 6\n",
    "    grouped = [np.empty((0, timeseries.shape[1])) for _ in range(num_goals)]\n",
    "    for idx, goal in enumerate(goals):\n",
    "        goal = int(goal - 1)\n",
    "        grouped[goal] = np.vstack((grouped[goal], timeseries[idx,:]))\n",
    "    return grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group cue phase spikes according to goal\n",
    "responses_per_goal = group_by_goal(binned_spikes_per_trial, trial_markers)\n",
    "\n",
    "# Get distribution of occurences of each goal\n",
    "num_responses_per_goal = np.array(list(map(lambda arr: arr.shape[0], responses_per_goal)))\n",
    "total_goal_responses = np.sum(num_responses_per_goal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get distribution of population responses across all cue phases\n",
    "response_distribution_cues = map_response_distribution_popl(binned_spikes_per_trial)\n",
    "response_distribution_cues_keys = list(response_distribution_cues.keys())\n",
    "total_unique_responses_cues = len(response_distribution_cues_keys)\n",
    "total_responses_cues = sum(response_distribution_cues.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get distribution of population responses aross entire session\n",
    "response_distribution_session = map_response_distribution_popl(binned_spikes_across_session)\n",
    "response_distribution_session_keys = list(response_distribution_session.keys())\n",
    "total_unique_responses_session = len(response_distribution_session_keys)\n",
    "total_responses_session = sum(response_distribution_session.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get distribution of population responses across entire session, accounting for different binning windows\n",
    "# response_distribution_extended = response_distribution_session.copy()\n",
    "# for i in range(1, 10):\n",
    "#     timeshift = 0.1 * i\n",
    "#     # Generate time intervals\n",
    "#     extended_intervals = np.arange(session_start_time + timeshift, session_end_time, 1)\n",
    "#     extended_intervals = np.vstack((extended_intervals[:-1], extended_intervals[1:])).T\n",
    "#     # Slot in spikes\n",
    "#     spikecounts = spike_counts_per_observation(extended_intervals, spike_times)\n",
    "#     # Bin spike counts\n",
    "#     binned_spikes = np.empty_like(spikecounts)\n",
    "#     for col in range(spikecounts.shape[1]):\n",
    "#         binned_spikes[:,col] = bin_firing_rates_3(spikecounts[:,col])\n",
    "#     # Add to response distribution map\n",
    "#     response_distribution_extended = map_response_distribution_popl(binned_spikes, dist=response_distribution_extended)\n",
    "\n",
    "# response_distriubtion_extended_keys = list(response_distribution_extended.keys())\n",
    "# total_unique_responses_extended = len(response_distriubtion_extended_keys)\n",
    "# total_responses_extended = sum(response_distribution_extended.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def information_per_stimulus(responses_per_stimulus: np.array, response_dist: dict) -> float:\n",
    "    responses_per_stimulus_dist = map_response_distribution_popl(responses_per_stimulus)\n",
    "    total_responses, total_responses_in_stimulus = sum(response_dist.values()), sum(responses_per_stimulus_dist.values())\n",
    "    res = 0\n",
    "    for response in responses_per_stimulus_dist:\n",
    "        P_r_given_s = responses_per_stimulus_dist[response] / total_responses_in_stimulus\n",
    "        P_r = response_dist.get(response, 1) / total_responses\n",
    "        res += P_r_given_s * np.log2(P_r_given_s / P_r)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.561812481597263\n"
     ]
    }
   ],
   "source": [
    "# (Unconditioned) Entropy across cue phases\n",
    "entropy_cue = 0\n",
    "for obs in response_distribution_cues:\n",
    "    P_r = response_distribution_cues[obs] / total_responses_cues\n",
    "    entropy_cue -= P_r * np.log2(P_r)\n",
    "\n",
    "print(entropy_cue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.737103540988251\n"
     ]
    }
   ],
   "source": [
    "# (Unconditioned) Entropy across entire session\n",
    "entropy_session = 0\n",
    "for obs in response_distribution_session:\n",
    "    P_r = response_distribution_session[obs] / total_responses_session\n",
    "    entropy_session -= P_r * np.log2(P_r)\n",
    "\n",
    "print(entropy_session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5.96875, 6.093409091335662, 6.234120167580205, 6.548548539271539, 6.044394119358462, 5.913787860007857]\n"
     ]
    }
   ],
   "source": [
    "# (Conditioned) Entropy across responses for each goal\n",
    "entropy_responses = list()\n",
    "for goal_responses in responses_per_goal:\n",
    "    entropy_goal = 0\n",
    "    goal_responses_dist = map_response_distribution_popl(goal_responses)\n",
    "    goal_responses_total = sum(goal_responses_dist.values())\n",
    "    for obs in goal_responses_dist:\n",
    "        P_r_s = goal_responses_dist[obs] / goal_responses_total\n",
    "        entropy_goal -= P_r_s * np.log2(P_r_s)\n",
    "    entropy_responses.append(entropy_goal)\n",
    "\n",
    "print(entropy_responses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3924284203679247\n"
     ]
    }
   ],
   "source": [
    "# Calculate information per stimulus for each goal (using responses from cue phase only)\n",
    "information_per_goal_cues = list()\n",
    "for goal in responses_per_goal:\n",
    "    information_per_goal_cues.append(information_per_stimulus(goal, response_distribution_cues))\n",
    "\n",
    "# Calculate mutual informtion across cues (using responses from cue phase only)\n",
    "goal_mutual_information_cues = 0\n",
    "for goal, info in enumerate(information_per_goal_cues):\n",
    "    P_s = num_responses_per_goal[goal] / total_goal_responses\n",
    "    goal_mutual_information_cues += P_s * info\n",
    "\n",
    "print(goal_mutual_information_cues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.557695341161491\n"
     ]
    }
   ],
   "source": [
    "# Calculate information per stimulus for each goal (using responses across entire session)\n",
    "information_per_goal_session = list()\n",
    "for goal in responses_per_goal:\n",
    "    information_per_goal_session.append(information_per_stimulus(goal, response_distribution_session))\n",
    "\n",
    "# Calculate mutual informtion across cues (using responses across entire session)\n",
    "goal_mutual_information_session = 0\n",
    "for goal, info in enumerate(information_per_goal_session):\n",
    "    P_s = num_responses_per_goal[goal] / total_goal_responses\n",
    "    goal_mutual_information_session += P_s * info\n",
    "\n",
    "print(goal_mutual_information_session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Calculate information per stimulus for each goal (using extended response distribution) map\n",
    "# information_per_goal_extended = list()\n",
    "# for goal in responses_per_goal:\n",
    "#     information_per_goal_extended.append(information_per_stimulus(goal, response_distribution_extended))\n",
    "\n",
    "# # Calculate mutual informtion across cues (using extended response distribution map)\n",
    "# goal_mutual_information_extended = 0\n",
    "# for goal, info in enumerate(information_per_goal_extended):\n",
    "#     P_s = num_responses_per_goal[goal] / total_goal_responses\n",
    "#     goal_mutual_information_extended += P_s * info\n",
    "\n",
    "# print(goal_mutual_information_extended)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.565364163013027\n"
     ]
    }
   ],
   "source": [
    "# Shannon entropy across goals\n",
    "goal_entropy = 0\n",
    "for goal in range(6):\n",
    "    P_s = num_responses_per_goal[goal] / total_goal_responses\n",
    "    goal_entropy -= P_s * np.log2(P_s)\n",
    "\n",
    "print(goal_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.584962500721156"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.log2(6)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('decoding')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c3fe2a2c8d7eb9520fbb1c58f5c7303a51cf224a80a092f134560e88ca87af0b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
