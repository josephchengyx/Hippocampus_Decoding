{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports and script-wide statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import h5py\n",
    "from scipy.io import loadmat\n",
    "import numpy as np\n",
    "from preproc import *\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from itertools import combinations\n",
    "from scipy.special import factorial\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from matplotlib.gridspec import GridSpec\n",
    "from matplotlib.ticker import StrMethodFormatter\n",
    "from matplotlib.lines import Line2D\n",
    "from matplotlib.patches import Rectangle\n",
    "from matplotlib.colors import ListedColormap\n",
    "from matplotlib.widgets import Slider\n",
    "from IPython.display import HTML\n",
    "import textwrap\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plt.rcParams['font.size'] = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose which days to include\n",
    "# day_list = ['20181105', '20181102', '20181101']\n",
    "\n",
    "# Or, read in list of days from txt file\n",
    "day_list = list()\n",
    "with open(f'data/combined/days.txt', 'r') as file:\n",
    "    for line in file:\n",
    "        day_list.append(line.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some common constants\n",
    "num_sess = len(day_list)\n",
    "num_goals = 6\n",
    "tbin_size = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in predefined list of cells\n",
    "good_cell_labels = list()\n",
    "with open('data/cell_list_hm.txt', 'r') as file:\n",
    "    for line in file:\n",
    "        line = line.strip().split('/')\n",
    "        good_cell_labels.append(f'{line[5]}ch{str(int(line[8][7:]))}c{str(int(line[9][4:]))}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing of session-level navigation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_by_pos_bins(timeseries: np.array, pos_bins: np.array) -> list:\n",
    "    num_bins = 1600\n",
    "    grouped = [np.empty((0, timeseries.shape[1])) for _ in range(num_bins)]\n",
    "    for idx, bin in enumerate(pos_bins):\n",
    "        bin = int(bin)\n",
    "        if bin == 0:\n",
    "            continue\n",
    "        bin -= 1\n",
    "        grouped[bin] = np.vstack((grouped[bin], timeseries[idx,:]))\n",
    "    return grouped\n",
    "\n",
    "def place_response_distribution(timeseries: np.array, pos_bins: np.array) -> dict:\n",
    "    dist = dict()\n",
    "    for idx, bin in enumerate(pos_bins):\n",
    "        bin = int(bin)\n",
    "        if bin == 0:\n",
    "            continue\n",
    "        if bin not in dist:\n",
    "            dist[bin] = list()\n",
    "        dist[bin].append(timeseries[idx])\n",
    "    dist = {key: np.array(sorted(val)) for key, val in dist.items()}\n",
    "    return dist\n",
    "\n",
    "def downsample_pos_binning(pos_bins: np.array) -> np.array:\n",
    "    def pos_coords_to_bins(coords: np.array) -> int:\n",
    "        # Converts positional (x, y) coordinates to bin numbers, returns an int\n",
    "        # Default maze dimensions\n",
    "        num_bins = 5\n",
    "        coord_min, size = -12.5, 25\n",
    "        bin_width = size / num_bins\n",
    "        x, y = coords\n",
    "        # Convert to row/column number for each axis\n",
    "        h, v = int(np.floor((x - coord_min)/bin_width)), int(np.floor((y - coord_min)/bin_width))\n",
    "        # Combine to get actual bin number\n",
    "        return v * num_bins + h + 1\n",
    "    \n",
    "    new_pos_bins = np.zeros_like(pos_bins)\n",
    "    for i, bin in enumerate(pos_bins):\n",
    "        x, y = pos_bins_to_coords(bin).flatten()\n",
    "        new_pos_bins[i] = pos_coords_to_bins((x, y))\n",
    "    return new_pos_bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save directory for data files\n",
    "prefix = \"/Volumes/Hippocampus/Data/picasso-misc/\"\n",
    "save_dir = \"data/placedist\"\n",
    "# Whether to overwrite preexisting files\n",
    "overwrite = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for day_dir in day_list:\n",
    "    if os.path.exists(f'{save_dir}/{day_dir}_data.pkl') and not overwrite:\n",
    "        continue\n",
    "\n",
    "    # Get list of cells under the day directory\n",
    "    os.system(f\"sh ~/Documents/neural_decoding/Hippocampus_Decoding/get_cells.sh {day_dir}\")\n",
    "    cell_list = list()\n",
    "    with open(\"cell_list.txt\", \"r\") as file:\n",
    "        for line in file.readlines():\n",
    "            cell_list.append(line.strip())\n",
    "    os.system(\"rm cell_list.txt\")\n",
    "\n",
    "    # Load data and extract spike times from all spiketrain.mat objects\n",
    "    spike_times = list()\n",
    "    cell_labels = list()\n",
    "    for cell_dir in cell_list:\n",
    "        try:\n",
    "            spk_file = loadmat(prefix + day_dir + \"/session01/\" + cell_dir + \"/spiketrain.mat\")\n",
    "        except NotImplementedError:\n",
    "            spk_file = h5py.File(prefix + day_dir + \"/session01/\" + cell_dir + \"/spiketrain.mat\")\n",
    "        except FileNotFoundError:\n",
    "            continue\n",
    "        spk = np.array(spk_file.get('timestamps')).flatten() # spike timestamps is loaded in as a column vector\n",
    "        spk /= 1000 # convert spike timestamps from msec to sec\n",
    "        spike_times.append(spk)\n",
    "        if isinstance(spk_file, h5py.File):\n",
    "            spk_file.close()\n",
    "        \n",
    "        cell_name = cell_dir.split('/')\n",
    "        array, channel, cell = cell_name[0][6:], cell_name[1][7:], cell_name[2][5:]\n",
    "        if channel[0] == '0':\n",
    "            channel = channel[1:]\n",
    "        cell_labels.append(f'a{array}/ch{channel}/c{cell}')\n",
    "\n",
    "    # Load data from vmpv.mat object\n",
    "    pv_file = h5py.File(prefix + day_dir + \"/session01/1vmpv.mat\")\n",
    "    pv = pv_file.get('pv').get('data')\n",
    "\n",
    "    # Extract session time and position bins, then convert spike timestamps to spiketrains and then to spike rates\n",
    "    place_intervals = get_place_intervals(pv)\n",
    "    pv_file.close()\n",
    "    spikerates = spike_rates_per_observation(place_intervals, spike_times)\n",
    "\n",
    "    # Set mindur and threshvel filter\n",
    "    min_dur = 0.05 # Minimum duration for an observation set to 50ms\n",
    "    thresh_vel = 1 # Set minimum velocity threshold to 1 unit/s\n",
    "\n",
    "    dur_intervals = place_intervals[:,1] - place_intervals[:,0]\n",
    "    max_dur = np.sqrt(2) * 0.625 / thresh_vel\n",
    "    valid_obs = np.where((dur_intervals > min_dur) & (dur_intervals <= max_dur))\n",
    "\n",
    "    # Apply mindur and threshvel filter\n",
    "    place_intervals = place_intervals[valid_obs]\n",
    "    spikerates = spikerates[valid_obs]\n",
    "    dur_intervals = dur_intervals[valid_obs]\n",
    "\n",
    "    # Downsample position binning from (40 x 40) to (5 x 5)\n",
    "    place_intervals[:,2] = downsample_pos_binning(place_intervals[:,2])\n",
    "\n",
    "    # Generate spiking distribution per position bin for each individual cell\n",
    "    place_responses_per_cell = list()\n",
    "    for cell in range(spikerates.shape[1]):\n",
    "        responses_per_bin = place_response_distribution(spikerates[:,cell], place_intervals[:,2])\n",
    "        place_responses_per_cell.append(responses_per_bin)\n",
    "    durations_per_place = place_response_distribution(dur_intervals, place_intervals[:,2])\n",
    "\n",
    "    ### Save processed data to pkl file ###\n",
    "    for i, label in enumerate(cell_labels):\n",
    "        label = label.split('/')\n",
    "        label[1] = label[1][2:]\n",
    "        if label[1][0] == '0':\n",
    "            label[1] = label[1][1:]\n",
    "        cell_labels[i] = f'{day_dir}ch{label[1]}{label[2]}'\n",
    "\n",
    "    data = {'place_responses_per_cell': place_responses_per_cell, 'durations_per_place': durations_per_place, 'cell_labels': cell_labels}\n",
    "    with open(f'{save_dir}/{day_dir}_data.pkl', 'wb') as file:\n",
    "        pickle.dump(data, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pseudopopulation place responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_place_responses = list()\n",
    "all_place_durations = list()\n",
    "all_cell_labels = list()\n",
    "\n",
    "for day in day_list:\n",
    "    with open(f'{save_dir}/{day}_data.pkl', 'rb') as file:\n",
    "        data = pickle.load(file)\n",
    "        num_sess_cells = len(data['cell_labels'])\n",
    "        all_place_responses.extend(data['place_responses_per_cell'])\n",
    "        for _ in range(num_sess_cells):\n",
    "            all_place_durations.append(data['durations_per_place'])\n",
    "        all_cell_labels.extend(data['cell_labels'])\n",
    "\n",
    "num_all_cells = len(all_cell_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in predefined list of cells\n",
    "good_cell_labels = list()\n",
    "with open('data/cell_list_pseudopop.txt', 'r') as file:\n",
    "    for line in file:\n",
    "        good_cell_labels.append(line.rstrip())\n",
    "\n",
    "# Filter out cells not in pseudopopulation list\n",
    "cell_filter = np.array([idx for idx, cell in enumerate(all_cell_labels) if cell in set(good_cell_labels)])\n",
    "all_place_responses = [all_place_responses[i] for i in cell_filter]\n",
    "all_place_durations = [all_place_durations[i] for i in cell_filter]\n",
    "\n",
    "# Update num_all_cells and all_cell_labels to reflect number of cells in pseudopopulation\n",
    "num_all_cells = cell_filter.shape[0]\n",
    "all_cell_labels = [all_cell_labels[i] for i in cell_filter]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert each place response distribution into (mean, var, duration) summary parameters\n",
    "for cell, place_responses_per_cell in enumerate(all_place_responses):\n",
    "    num_bins = 25\n",
    "    response_params_per_cell = np.zeros((num_bins+1, 3))\n",
    "    place_durations_per_cell = all_place_durations[cell]\n",
    "    for bin, dist in place_responses_per_cell.items():\n",
    "        response_params_per_cell[bin,0] = np.mean(dist)\n",
    "        response_params_per_cell[bin,1] = np.std(dist, ddof=1)\n",
    "        response_params_per_cell[bin,2] = np.sum(place_durations_per_cell[bin])\n",
    "    response_params_per_cell[:,2] = response_params_per_cell[:,2] / np.sum(response_params_per_cell[:,2])\n",
    "    all_place_responses[cell] = response_params_per_cell\n",
    "all_place_responses = np.array(all_place_responses)\n",
    "\n",
    "# Clean up large memory variables\n",
    "del all_place_durations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(233, 26, 3)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_place_responses.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Place response distributions plots per cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_save = True\n",
    "figsave_dir = 'figures/placedist'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "for day in day_list:\n",
    "    with open(f'{save_dir}/{day}_data.pkl', 'rb') as file:\n",
    "        data = pickle.load(file)\n",
    "        place_responses_per_cell = data['place_responses_per_cell']\n",
    "        cell_labels = data['cell_labels']\n",
    "\n",
    "    for cell, label in enumerate(cell_labels):\n",
    "        if label not in good_cell_labels:\n",
    "            continue\n",
    "        \n",
    "        fig = plt.figure(figsize=(20, 12))\n",
    "        plt.subplots_adjust(left=0.1, right=0.9, top=0.9, bottom=0.1, wspace=0.2, hspace=0.35)\n",
    "        plt.suptitle(f'Firing rate distribution per place bin for cell {label}', y=0.95, fontsize=14)\n",
    "\n",
    "        for bin, dist in place_responses_per_cell[cell].items():\n",
    "            # Get distribution median and mean\n",
    "            dist_mean, dist_median = np.mean(dist), np.median(dist)\n",
    "            # Filter out 0 Hz observations\n",
    "            dist = dist[dist > 0]\n",
    "            # Plot subplots in same position as actual place bins in the environment\n",
    "            fig_pos = (4 - (bin - 1) // 5) * 5 + ((bin - 1) % 5) + 1\n",
    "\n",
    "            ax = fig.add_subplot(5, 5, fig_pos)\n",
    "            ax.set_title(f'Bin #{bin}', fontsize=12)\n",
    "            ax.hist(dist, bins=50)\n",
    "            ax.axvline(x=dist_mean, color='C1', linewidth=1, linestyle='--')\n",
    "            ax.axvline(x=dist_median, color='green', linewidth=1, linestyle='--')\n",
    "            ax.set_xlim(0, ax.get_xlim()[1])\n",
    "            if fig_pos == 23:\n",
    "                ax.set_xlabel('Firing rate (Hz)', fontsize=14)\n",
    "            if fig_pos == 11:\n",
    "                ax.set_ylabel('Count', fontsize=14)\n",
    "        \n",
    "        if to_save:\n",
    "            if not os.path.exists(figsave_dir):\n",
    "                os.makedirs(figsave_dir)\n",
    "            plt.savefig(f'{figsave_dir}/placedist_{label}.png', bbox_inches='tight')\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bayesian place decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BayesDecoder:\n",
    "    def __init__(self, dist):\n",
    "        self.dist = dist\n",
    "        self.num_cells = dist.shape[0]\n",
    "        self.num_bins = dist.shape[1]\n",
    "\n",
    "    def gaussian_pdf(x, mu, sig):\n",
    "        return (1 / np.sqrt(2 * np.pi * sig**2)) * np.exp(-(x - mu)**2 / (2 * sig**2))\n",
    "    \n",
    "    def __likelihood(self, x, cell, bin):\n",
    "        mu, sig = self.dist[cell,bin,0], self.dist[cell,bin,1]\n",
    "        return BayesDecoder.gaussian_pdf(x, mu, sig)\n",
    "    \n",
    "    def __predict_cell(self, x, cell):\n",
    "        posterior = np.zeros(self.num_bins)\n",
    "        for bin in range(1, self.num_bins):\n",
    "            prior = self.dist[cell,bin,2]\n",
    "            posterior[bin] = prior * self.__likelihood(x, cell, bin)\n",
    "        pred = np.argmax(posterior[1:])\n",
    "        return pred, posterior[pred] \n",
    "\n",
    "    def predict(self, x):\n",
    "        prediction, confidence = np.zeros(self.num_cells), np.zeros(self.num_cells)\n",
    "        for cell in range(self.num_cells):\n",
    "            pred, conf = self.__predict_cell(x[cell], cell)\n",
    "            prediction[cell] = pred\n",
    "            confidence[cell] = conf\n",
    "        prediction, counts = np.unique(prediction, return_counts=True)\n",
    "        return prediction[np.argmax(counts)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "place_decoder = BayesDecoder(all_place_responses)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "decoding",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
